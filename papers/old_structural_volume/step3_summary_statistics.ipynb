{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "green",
   "display_name": "green",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Summary Statistics Approach"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local path, change this.\n",
    "yellow_follow = 'C:/Users/caspe/Desktop/yellow/lib/'\n",
    "\n",
    "import sys; sys.path.append(yellow_follow) \n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import ml_utils\n",
    "import numpy as np\n",
    "\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import Huber, MSLE\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "source": [
    "# Load data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "folder = \"C:/Users/caspe/Desktop/Paper_2_StructuralVolume/\"\n",
    "in_path = folder + \"grid.sqlite\"\n",
    "\n",
    "db_cnx = sqlite3.connect(in_path)\n",
    "df = pd.read_sql_query(\"SELECT * FROM 'grid';\", db_cnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy reference to the different features in the datasets.\n",
    "s2 = [\n",
    "    'b04_mean', 'b04_stdev', 'b04_min', 'b04_max',\n",
    "    'b08_mean', 'b08_stdev', 'b08_min', 'b08_max',\n",
    "    'b04t_mean', 'b04t_stdev', 'b04t_min', 'b04t_max',\n",
    "    'b08t_mean', 'b08t_stdev', 'b08t_min', 'b08t_max',\n",
    "]\n",
    "\n",
    "bs_asc = ['bs_asc_mean', 'bs_asc_stdev', 'bs_asc_min', 'bs_asc_max']\n",
    "bs_desc = ['bs_desc_mean', 'bs_desc_stdev', 'bs_desc_min', 'bs_desc_max']\n",
    "coh_asc = ['coh_asc_mean', 'coh_asc_stdev', 'coh_asc_min', 'coh_asc_max']\n",
    "coh_desc = ['coh_desc_mean', 'coh_desc_stdev', 'coh_desc_min', 'coh_desc_max']\n",
    "\n",
    "nl = ['nl_mean', 'nl_stdev', 'nl_min', 'nl_max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The municipalities used as test targets\n",
    "#                       Rural,                  Mix,    Dense-urban\n",
    "test_municipalities = ['Lemvig', 'Silkeborg', 'Aarhus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'volume'\n",
    "# target = 'people'\n",
    "# target = 'area'"
   ]
  },
  {
   "source": [
    "# Define the neural network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "def define_model(shape, name):\n",
    "    model_input = Input(shape=shape, name=\"input\")\n",
    "    model = Dense(1024, activation=tfa.activations.mish, kernel_initializer=\"he_normal\")(model_input)\n",
    "    model = Dropout(0.2)(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Dense(256, activation=tfa.activations.mish, kernel_initializer=\"he_normal\")(model)\n",
    "    model = Dropout(0.2)(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Dense(64, activation=tfa.activations.mish, kernel_initializer=\"he_normal\")(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Dense(16, activation=tfa.activations.mish, kernel_initializer=\"he_normal\")(model)\n",
    "    model = BatchNormalization()(model)\n",
    "\n",
    "    predictions = Dense(1, activation='relu')(model)\n",
    "\n",
    "    return Model(inputs=[model_input], outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimizer\n",
    "def define_optimizer():\n",
    "    return tfa.optimizers.Lookahead(\n",
    "        Adam(\n",
    "            learning_rate=tfa.optimizers.TriangularCyclicalLearningRate(\n",
    "                initial_learning_rate=1e-5,\n",
    "                maximal_learning_rate=1e-2,\n",
    "                step_size=9,\n",
    "                scale_mode='cycle',\n",
    "                name='TriangularCyclicalLearningRate',\n",
    "            ),\n",
    "            name=\"Adam\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "source": [
    "# Start analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_to_run = [\n",
    "    # nl,\n",
    "    # s2,\n",
    "    # bs_asc,\n",
    "    # bs_desc,\n",
    "    # bs_asc + bs_desc, \n",
    "    # bs_asc + coh_asc,\n",
    "    # bs_desc + coh_desc,\n",
    "    # bs_asc + coh_asc + s2,\n",
    "    # bs_asc + coh_asc + bs_desc + coh_desc,\n",
    "    bs_asc + coh_asc + bs_desc + coh_desc + s2,\n",
    "    # bs_asc + coh_asc + bs_desc + coh_desc + s2 + nl,\n",
    "]\n",
    "analysis_name = 'bsac_bsdc_s2'"
   ]
  },
  {
   "source": [
    "# Train and evaluate"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "741/741 [==============================] - ETA: 0s - loss: 1793.1260 - mean_absolute_error: 1793.1260 - mean_absolute_percentage_error: 1028704683.9190 - median_absolute_error: 0.0303 - median_absolute_percentage_error: 3603207023.2585"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-e3a94984aa6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         model.fit(\n\u001b[0m\u001b[0;32m     39\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\green\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1129\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[1;32m-> 1131\u001b[1;33m           val_logs = self.evaluate(\n\u001b[0m\u001b[0;32m   1132\u001b[0m               \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m               \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\green\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1387\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1388\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1389\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1390\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1391\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\green\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\green\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    860\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\green\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\green\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\green\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\green\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_analysis = []\n",
    "\n",
    "for idx, analysis in enumerate(analysis_to_run):\n",
    "\n",
    "    scores = { \"analysis\": str(analysis) }\n",
    "\n",
    "    for muni in test_municipalities:\n",
    "        train = df[df['muni_name'] != muni]\n",
    "        test = df[df['muni_name'] == muni]\n",
    "\n",
    "        muni_code = str(int(test['muni_code'].iloc[0]))\n",
    "        \n",
    "        X_train = train[analysis].values\n",
    "        X_test = test[analysis].values\n",
    "\n",
    "        y_train = train[target].values\n",
    "        y_test = test[target].values\n",
    "\n",
    "        shape = X_train.shape[1]\n",
    "        model = define_model(shape, \"input\")\n",
    "\n",
    "        # Compile and test model\n",
    "        model.compile(\n",
    "            optimizer=define_optimizer(),\n",
    "            loss=\"mean_absolute_error\",\n",
    "            metrics=[\n",
    "                \"mean_absolute_error\",\n",
    "                \"mean_absolute_percentage_error\",\n",
    "                ml_utils.median_absolute_error,\n",
    "                ml_utils.median_absolute_percentage_error,\n",
    "            ])\n",
    "        \n",
    "\n",
    "        plot_model(model, to_file=\"./summary_model.png\", show_shapes=True, show_dtype=False, show_layer_names=False)\n",
    "\n",
    "        exit()\n",
    "\n",
    "        model.fit(\n",
    "            x=X_train,\n",
    "            y=y_train,\n",
    "            epochs=100,\n",
    "            verbose=1,\n",
    "            batch_size=2048,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[\n",
    "                EarlyStopping(\n",
    "                    monitor=\"val_loss\",\n",
    "                    patience=9,\n",
    "                    min_delta=5.0,\n",
    "                    restore_best_weights=True,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Evaluate model\n",
    "        loss, mae, mape, meae, meape = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "        scores[muni] = {\n",
    "            \"mean_absolute_error\": mae,\n",
    "            \"mean_absolute_percentage_error\": mape,\n",
    "            \"median_absolute_error\": meae,\n",
    "            \"median_absolute_percentage_error\": meape,\n",
    "        }\n",
    "\n",
    "        # Save the predicted values\n",
    "        pred = model.predict(X_test)\n",
    "        test[f\"pred_{analysis_name}_{muni_code}\"] = pred\n",
    "\n",
    "        engine = create_engine(f\"sqlite:///{folder}grid_{analysis_name}_{muni_code}.sqlite\", echo=True)\n",
    "        sqlite_connection = engine.connect()\n",
    "\n",
    "        test.to_sql(f\"grid_{analysis_name}_{muni_code}\", sqlite_connection, if_exists='fail')\n",
    "        sqlite_connection.close()\n",
    "\n",
    "\n",
    "    all_analysis.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, analysis in enumerate(all_analysis):\n",
    "    print(f\"Analysis: {analysis_name}\")\n",
    "    print(\"\")\n",
    "\n",
    "    combined = {\n",
    "        \"MAE\": [],\n",
    "        \"MAPE\": [],\n",
    "        \"MeAE\": [],\n",
    "        \"MeAPE\": [],\n",
    "    }\n",
    "\n",
    "    for munipality in test_municipalities:\n",
    "        test_area_name = munipality\n",
    "        test_data = analysis[test_area_name]\n",
    "\n",
    "        mean_absolute_error = test_data[\"mean_absolute_error\"]\n",
    "        mean_absolute_percentage_error = test_data[\"mean_absolute_percentage_error\"]\n",
    "        median_absolute_error = test_data[\"median_absolute_error\"]\n",
    "        median_absolute_percentage_error = test_data[\"median_absolute_percentage_error\"]\n",
    "\n",
    "        combined[\"MAE\"].append(mean_absolute_error)\n",
    "        combined[\"MAPE\"].append(mean_absolute_percentage_error)\n",
    "        combined[\"MeAE\"].append(median_absolute_error)\n",
    "        combined[\"MeAPE\"].append(median_absolute_percentage_error)\n",
    "\n",
    "        print(f\"    Test area: {test_area_name}\")\n",
    "        print(f\"    Mean Absolute Error (MAE):                {ml_utils.pad(str(round(mean_absolute_error, 3)), 5, 3)}\")\n",
    "        print(f\"    Mean Absolute Percentage Error (MAPE):    {ml_utils.pad(str(round(mean_absolute_percentage_error * 100, 3)), 5, 3)}\")\n",
    "        print(f\"    Median Absolute Error (MeAE):             {ml_utils.pad(str(round(median_absolute_error, 3)), 5, 3)}\")\n",
    "        print(f\"    Median Absolute Percentage Error (MeAPE): {ml_utils.pad(str(round(median_absolute_percentage_error * 100, 3)), 5, 3)}\")\n",
    "        print(\"\")\n",
    "    \n",
    "    mae_mean = np.array(combined['MAE']).mean()\n",
    "    mae_std = np.array(combined['MAE']).std()\n",
    "\n",
    "    mape_mean = np.array(combined['MAPE']).mean()\n",
    "    mape_std = np.array(combined['MAPE']).std()\n",
    "\n",
    "    meae_mean = np.array(combined['MeAE']).mean()\n",
    "    meae_std = np.array(combined['MeAE']).std()\n",
    "\n",
    "    meape_mean = np.array(combined['MeAPE']).mean()\n",
    "    meapee_std = np.array(combined['MeAPE']).std()\n",
    "\n",
    "\n",
    "    print(f\"    Combined Score:\")\n",
    "    print(f\"    Mean Absolute Error (MAE):               {ml_utils.pad(str(round(mae_mean, 3)), 5, 3)} ({ml_utils.pad(str(round(mae_mean, 3)), 5, 3)} σ)\")\n",
    "    print(f\"    Mean Absolute Percentage Error (MAPE):   {ml_utils.pad(str(round(mape_mean * 100, 3)), 5, 3)} ({ml_utils.pad(str(round(mape_std * 100, 3)), 5, 3)} σ)\")\n",
    "    print(f\"    Median Absolute Error (MeAE):            {ml_utils.pad(str(round(meae_mean, 3)), 5, 3)} ({ml_utils.pad(str(round(meae_std, 3)), 5, 3)} σ)\")\n",
    "    print(f\"    Median Absolute Percentage Error (MAPE): {ml_utils.pad(str(round(meape_mean * 100, 3)), 5, 3)} ({ml_utils.pad(str(round(meapee_std * 100, 3)), 5, 3)} σ)\")\n",
    "    print(\"\")"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}